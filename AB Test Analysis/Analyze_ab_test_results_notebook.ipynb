{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze A/B Test Results\n",
    "\n",
    "This project will assure you have mastered the subjects covered in the statistics lessons.  The hope is to have this project be as comprehensive of these topics as possible.  Good luck!\n",
    "\n",
    "## Table of Contents\n",
    "- [Introduction](#intro)\n",
    "- [Part I - Probability](#probability)\n",
    "- [Part II - A/B Test](#ab_test)\n",
    "- [Part III - Regression](#regression)\n",
    "\n",
    "\n",
    "<a id='intro'></a>\n",
    "### Introduction\n",
    "\n",
    "A/B tests are very commonly performed by data analysts and data scientists.  It is important that you get some practice working with the difficulties of these \n",
    "\n",
    "For this project, you will be working to understand the results of an A/B test run by an e-commerce website.  Your goal is to work through this notebook to help the company understand if they should implement the new page, keep the old page, or perhaps run the experiment longer to make their decision.\n",
    "\n",
    "**As you work through this notebook, follow along in the classroom and answer the corresponding quiz questions associated with each question.** The labels for each classroom concept are provided for each question.  This will assure you are on the right track as you work through the project, and you can feel more confident in your final submission meeting the criteria.  As a final check, assure you meet all the criteria on the [RUBRIC](https://review.udacity.com/#!/projects/37e27304-ad47-4eb0-a1ab-8c12f60e43d0/rubric).\n",
    "\n",
    "<a id='probability'></a>\n",
    "#### Part I - Probability\n",
    "\n",
    "To get started, let's import our libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline      \n",
    "\n",
    "# Setting the seed to get the same answers on each run\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1.` Now, read in the `ab_data.csv` data. Store it in `df`.  **Use your dataframe to answer the questions in Quiz 1 of the classroom.**\n",
    "\n",
    "a. Read in the dataset and take a look at the top few rows here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Datasets\n",
    "\n",
    "ab_test_data = pd.read_csv('ab_data.csv')\n",
    "country_data = pd.read_csv('countries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Preview\n",
    "\n",
    "ab_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Use the below cell to find the number of rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 294478\n"
     ]
    }
   ],
   "source": [
    "# Number of rows\n",
    "\n",
    "num_rows = ab_test_data.shape[0]\n",
    "print(f'Number of rows: {num_rows}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. The number of unique users in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 290584\n"
     ]
    }
   ],
   "source": [
    "# Number of unique users\n",
    "\n",
    "num_unique_users = ab_test_data.user_id.nunique()\n",
    "print(f'Number of unique users: {num_unique_users}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "d. The proportion of users converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proportion of users converted: 0.11965919355605512\n"
     ]
    }
   ],
   "source": [
    "# Conversion rate\n",
    "\n",
    "conversion_rate = ab_test_data.converted.mean()\n",
    "print(f'The proportion of users converted: {conversion_rate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "e. The number of times the `new_page` and `treatment` don't line up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times group and page don't line up: 3893\n"
     ]
    }
   ],
   "source": [
    "# Page group mismatch\n",
    "\n",
    "control_got_new_page = ab_test_data.query('group == \"control\" and landing_page == \"new_page\"').shape[0]\n",
    "\n",
    "treatment_got_old_page = ab_test_data.query('group == \"treatment\" and landing_page == \"old_page\"').shape[0]\n",
    "\n",
    "\n",
    "group_page_mismatch =  control_got_new_page + treatment_got_old_page\n",
    "\n",
    "print(f'Number of times group and page don\\'t line up: {group_page_mismatch}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Do any of the rows have missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value information\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user_id         0\n",
       "timestamp       0\n",
       "group           0\n",
       "landing_page    0\n",
       "converted       0\n",
       "Name: Missing value information, dtype: int64"
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "\n",
    "missing_values = ab_test_data.isnull().sum()\n",
    "missing_values.name = 'Missing value information'\n",
    "\n",
    "print(missing_values.name)\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` For the rows where **treatment** is not aligned with **new_page** or **control** is not aligned with **old_page**, we cannot be sure if this row truly received the new or old page.  Use **Quiz 2** in the classroom to provide how we should handle these rows.  \n",
    "\n",
    "a. Now use the answer to the quiz to create a new dataset that meets the specifications from the quiz.  Store your new dataframe in **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling rows with group-page mismatch\n",
    "\n",
    "# Extracting the indices of problematic rows\n",
    "\n",
    "control_got_new_page_idx =   ab_test_data.query('group == \"control\" and landing_page == \"new_page\"').index.tolist()\n",
    "\n",
    "treatment_got_old_page_idx = ab_test_data.query('group == \"treatment\" and landing_page == \"old_page\"').index.tolist()\n",
    "\n",
    "\n",
    "group_page_mismatch_idx = pd.Index(control_got_new_page_idx + treatment_got_old_page_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping problematic rows\n",
    "\n",
    "ab_data = ab_test_data.drop(index = group_page_mismatch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking if the rows were correctly removed\n",
    "\n",
    "ab_data[((ab_data['group'] == 'treatment') == (ab_data['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`3.` Use **df2** and the cells below to answer questions for **Quiz3** in the classroom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. How many unique **user_id**s are in **df2**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users in transformed data: 290584\n"
     ]
    }
   ],
   "source": [
    "# Unique users in ab_data\n",
    "\n",
    "num_unique_users =  ab_data.user_id.nunique()\n",
    "print(f'Number of unique users in transformed data: {num_unique_users}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b. There is one **user_id** repeated in **df2**.  What is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id of repeated users: 773192\n"
     ]
    }
   ],
   "source": [
    "# Checking for repeated users\n",
    "\n",
    "repeated_users = ab_data.user_id.value_counts().where(ab_data.user_id.value_counts() > 1).dropna().index[0]\n",
    "\n",
    "print(f'Id of repeated users: {repeated_users}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What is the row information for the repeat **user_id**? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row Information of Repeated User\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Row information of repeated user\n",
    "\n",
    "repeated_user_data =  ab_data.query('user_id == @repeated_users')\n",
    "\n",
    "print(\"Row Information of Repeated User\")\n",
    "repeated_user_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Remove **one** of the rows with a duplicate **user_id**, but keep your dataframe as **df2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping the reduntant \n",
    "\n",
    "ab_data.drop(repeated_user_data.index[0], inplace = True)\n",
    "\n",
    "# Final Preview\n",
    "\n",
    "ab_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`4.` Use **df2** in the below cells to answer the quiz questions related to **Quiz 4** in the classroom.\n",
    "\n",
    "a. What is the probability of an individual converting regardless of the page they receive?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion ratio agnostic of landing page version: 0.11959708724499628\n"
     ]
    }
   ],
   "source": [
    "# Calculating the overall conversion ratio\n",
    "\n",
    "conversion_ratio = ab_data.converted.mean()\n",
    "print(f'Conversion ratio agnostic of landing page version: {conversion_ratio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Given that an individual was in the `control` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probablity of conversion in control group: 0.1203863045004612\n"
     ]
    }
   ],
   "source": [
    "# Conversion ratio in control group\n",
    "\n",
    "control_group_conversions = ab_data.query('group == \"control\"').converted.mean()\n",
    "\n",
    "print(f'Probablity of conversion in control group: {control_group_conversions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Given that an individual was in the `treatment` group, what is the probability they converted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probablity of conversion in treatment group: 0.11880806551510564\n"
     ]
    }
   ],
   "source": [
    "# Conversion ratio in treatment group\n",
    "\n",
    "treatment_group_conversions = ab_data.query('group == \"treatment\"').converted.mean()\n",
    "   \n",
    "print(f'Probablity of conversion in treatment group: {treatment_group_conversions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is the probability that an individual received the new page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probablity of an recieving the new version of landing page: 0.5000619442226688\n"
     ]
    }
   ],
   "source": [
    "# Proportion of users recieving the new landing page\n",
    "\n",
    "new_page_user_ratio = ab_data.query('landing_page == \"new_page\"').shape[0]/ab_data.shape[0]\n",
    "\n",
    "print(f'Probablity of an recieving the new version of landing page: {new_page_user_ratio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Consider your results from a. through d. above, and explain below whether you think there is sufficient evidence to say that the new treatment page leads to more conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inference from Calculations**\n",
    "\n",
    "The calculations show that the control group has had a higher conversion ratio than the treatment group suggesting the new page is ineffective in driving more conversions. This result, however, needs to be tested for statistical significance since we have only worked with a sample of the population. Thus, I think there isn't sufficient evidence to explain the performance of the new page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "\n",
    "Notice that because of the time stamp associated with each event, you could technically run a hypothesis test continuously as each observation was observed.  \n",
    "\n",
    "However, then the hard question is do you stop as soon as one page is considered significantly better than another or does it need to happen consistently for a certain amount of time?  How long do you run to render a decision that neither page is better than another?  \n",
    "\n",
    "These questions are the difficult parts associated with A/B tests in general.  \n",
    "\n",
    "\n",
    "`1.` For now, consider you need to make the decision just based on all the data provided.  If you want to assume that the old page is better unless the new page proves to be definitely better at a Type I error rate of 5%, what should your null and alternative hypotheses be?  You can state your hypothesis in terms of words or in terms of **$p_{old}$** and **$p_{new}$**, which are the converted rates for the old and new pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hypotheses in Words*:\n",
    "\n",
    "Null Hypothesis: \n",
    "\n",
    "The proportion of users converted in the treatment group is **less than or equal** to the proportion of users converted in the control group. \n",
    "\n",
    "Alternate Hypothesis: \n",
    "\n",
    "The proportion of users converted in the treatment group is **greater than** the proportion of users converted in the treatment group. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hypotheses in Notation*:\n",
    "\n",
    "Null Hypothesis:     \n",
    "\n",
    "$p_{new}$ - $p_{old}$ $≤$  $0$\n",
    "\n",
    "\n",
    "Alternate Hypothesis:\n",
    "\n",
    "$p_{new}$ - $p_{old}$ $>$  $0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`2.` Assume under the null hypothesis, $p_{new}$ and $p_{old}$ both have \"true\" success rates equal to the **converted** success rate regardless of page - that is $p_{new}$ and $p_{old}$ are equal. Furthermore, assume they are equal to the **converted** rate in **ab_data.csv** regardless of the page. <br><br>\n",
    "\n",
    "Use a sample size for each page equal to the ones in **ab_data.csv**.  <br><br>\n",
    "\n",
    "Perform the sampling distribution for the difference in **converted** between the two pages over 10,000 iterations of calculating an estimate from the null.  <br><br>\n",
    "\n",
    "Use the cells below to provide the necessary parts of this simulation.  If this doesn't make complete sense right now, don't worry - you are going to work through the problems below to complete this problem.  You can use **Quiz 5** in the classroom to make sure you are on the right track.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. What is the **convert rate** for $p_{new}$ under the null? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion rate for new page under the null: 0.11959708724499628\n"
     ]
    }
   ],
   "source": [
    "# p_new under the null\n",
    "\n",
    "p_new = conversion_ratio\n",
    "\n",
    "print(f'Conversion rate for new page under the null: {p_new}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. What is the **convert rate** for $p_{old}$ under the null? <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion rate for old page under the null: 0.11959708724499628\n"
     ]
    }
   ],
   "source": [
    "# p_old under the null \n",
    "\n",
    "p_old = conversion_ratio\n",
    "\n",
    "print(f'Conversion rate for old page under the null: {p_old}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. What is $n_{new}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of individuals in the treatment group: 145310\n"
     ]
    }
   ],
   "source": [
    "# Number of individuals in treatment group\n",
    "\n",
    "n_new = ab_data.query('group == \"treatment\"').shape[0]\n",
    "\n",
    "print(f'Number of individuals in the treatment group: {n_new}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. What is $n_{old}$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of individuals in control group: 145274\n"
     ]
    }
   ],
   "source": [
    "# Number of indviduals in control group\n",
    "\n",
    "n_old = ab_data.query('group == \"control\"').shape[0]\n",
    "\n",
    "print(f'Number of individuals in control group: {n_old}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Simulate $n_{new}$ transactions with a convert rate of $p_{new}$ under the null.  Store these $n_{new}$ 1's and 0's in **new_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion ratio for new page under null simulation: 0.8787557635400179\n"
     ]
    }
   ],
   "source": [
    "# New page conversion ratio under null simulation\n",
    "\n",
    "''' Reference for the next line of code: \n",
    "    https://github.com/Abhishek20182/Analyze-AB-Test-Results/blob/master/Analyze_ab_test_results_notebook.ipynb\n",
    "    under the MIT License  '''\n",
    "\n",
    "new_page_converted = np.random.choice(2, n_new, p = [p_new, 1- p_new])\n",
    "\n",
    "print(f'Conversion ratio for new page under null simulation: {new_page_converted.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Simulate $n_{old}$ transactions with a convert rate of $p_{old}$ under the null.  Store these $n_{old}$ 1's and 0's in **old_page_converted**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion ratio for old page under null simulation: 0.8803020499194625\n"
     ]
    }
   ],
   "source": [
    "# Old page converion ratio under null simulation\n",
    "\n",
    "old_page_converted = np.random.choice(2,n_old, p = [p_old, 1 - p_old])\n",
    "\n",
    "print(f'Conversion ratio for old page under null simulation: {old_page_converted.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Find $p_{new}$ - $p_{old}$ for your simulated values from part (e) and (f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference under null simulation: -0.0015462863794445392\n"
     ]
    }
   ],
   "source": [
    "# Observed difference under the null simulation\n",
    "\n",
    "observed_diff_in_simulation = new_page_converted.mean() -  old_page_converted.mean()\n",
    "\n",
    "print(f'Observed difference under null simulation: {observed_diff_in_simulation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Simulate 10,000 $p_{new}$ - $p_{old}$ values using this same process similarly to the one you calculated in parts **a. through g.** above.  Store all 10,000 values in a numpy array called **p_diffs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducting 10000 simulations under the null hypothesis\n",
    "\n",
    "p_diffs = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    \n",
    "    simulated_new_page_converted = np.random.choice(2, n_new, p = [p_new, 1 - p_new])\n",
    "    simulated_old_page_converted = np.random.choice(2, n_new, p = [p_old, 1- p_old])\n",
    "    \n",
    "    diff = simulated_new_page_converted.mean() - simulated_old_page_converted.mean()\n",
    "    \n",
    "    p_diffs.append(diff)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Plot a histogram of the **p_diffs**.  Does this plot look like what you expected?  Use the matching problem in the classroom to assure you fully understand what was computed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference from user data: -0.0015782389853555567\n"
     ]
    }
   ],
   "source": [
    "# Computing the observed difference from experiment data\n",
    "\n",
    "observed_difference = treatment_group_conversions - control_group_conversions\n",
    "\n",
    "print(f'Observed difference from user data: {observed_difference}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot of difference in conversion rates from the simulations\n",
    "\n",
    "plt.hist(p_diffs);\n",
    "plt.axvline(observed_difference, color = 'red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j. What proportion of the **p_diffs** are greater than the actual difference observed in **ab_data.csv**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value: 0.9081\n"
     ]
    }
   ],
   "source": [
    "# Calculating the p-value\n",
    " \n",
    "p_value = np.sum([True if diff > observed_difference else False for diff in p_diffs])/len(p_diffs)\n",
    "\n",
    "print(f'p-value: {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k. In words, explain what you just computed in part **j.**  What is this value called in scientific studies?  What does this value mean in terms of whether or not there is a difference between the new and old pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation & Intepretation of p-value**  \n",
    "\n",
    "We calculated the probablity of finding an observation in favour of the alternate hypothesis given the null hypothesis is true. In this case, it was the mean of all values to the right of our observation.  In scientfic studies, this value is called the p-value - it is a conditional probablity of observing a certain statistic given the null hypothesis is true.  \n",
    "\n",
    "The p-value is used to evaluate the statistical significance of results by comparing it with a certain significance level.  In this case, our p-value exceeds our significance level of 0.05 and therefore we fail to reject the null hypothesis. \n",
    "\n",
    "This suggests that there is no difference between the conversion rates of the new & old pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l. We could also use a built-in to achieve similar results.  Though using the built-in might be easier to code, the above portions are a walkthrough of the ideas that are critical to correctly thinking about statistical significance. Fill in the below to calculate the number of conversions for each page, as well as the number of individuals who received each page. Let `n_old` and `n_new` refer the the number of rows associated with the old page and new pages, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conversions in  control group: 17489\n",
      "Number of conversions in treatment group: 17264\n",
      "Number of users who were shown the old page: 145274\n",
      "Number of user who were shown the new page: 145310\n"
     ]
    }
   ],
   "source": [
    "# Calculating parameters for use in builtin function\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "convert_old = ab_data.query('landing_page == \"old_page\"')['converted'].sum()\n",
    "convert_new = ab_data.query('landing_page == \"new_page\"')['converted'].sum()\n",
    "\n",
    "n_old =  ab_data.query('landing_page == \"old_page\"').shape[0]\n",
    "n_new =  ab_data.query('landing_page == \"new_page\"').shape[0]\n",
    "\n",
    "print(f'Number of conversions in  control group: {convert_old}')\n",
    "print(f'Number of conversions in treatment group: {convert_new}')\n",
    "print(f'Number of users who were shown the old page: {n_old}')\n",
    "print(f'Number of user who were shown the new page: {n_new}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "m. Now use `stats.proportions_ztest` to compute your test statistic and p-value.  [Here](http://knowledgetack.com/python/statsmodels/proportions_ztest/) is a helpful link on using the built in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z-score: -1.3109241984234394\n",
      "p-value: 0.9050583127590245\n"
     ]
    }
   ],
   "source": [
    "# Calculating z-score & p-value\n",
    "\n",
    "z_score, p_value =   sm.stats.proportions_ztest([convert_new, convert_old], [n_new, n_old], alternative = 'larger')\n",
    "\n",
    "print(f'z-score: {z_score}')\n",
    "print(f'p-value: {p_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n. What do the z-score and p-value you computed in the previous question mean for the conversion rates of the old and new pages?  Do they agree with the findings in parts **j.** and **k.**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussing z-score & Comparing p-value Obtained Earlier**\n",
    "\n",
    "The z-score is measure of how many standard deviations away the observed value is from the mean of the standard normal distribution. The p-value measures the probablity of finding an observation that many standard deviations away from the mean. \n",
    "\n",
    "In our context, the z score measured how many standard deviations away our observed difference was from the null hypothesized difference of zero and the p-value measured the probablity of finding a value beyond the \n",
    "z-score.\n",
    "\n",
    "The p-value calculated using the built-in function is 0.90 which is greater than our type 1 error rate threshold of 0.05 \n",
    "indicating that our results are not statistically significant.\n",
    "\n",
    "Therefore the p-values calculated in the 2 parts agree with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "### Part III - A regression approach\n",
    "\n",
    "`1.` In this final part, you will see that the result you achieved in the A/B test in Part II above can also be achieved by performing regression.<br><br> \n",
    "\n",
    "a. Since each row is either a conversion or no conversion, what type of regression should you be performing in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since oour output varibale is categorical, we should be performing a **logistic regression**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. The goal is to use **statsmodels** to fit the regression model you specified in part **a.** to see if there is a significant difference in conversion based on which page a customer receives.  However, you first need to create a column for the intercept, and create a dummy variable column for which page each user received.  Add an **intercept** column, as well as an **ab_page** column, which is 1 when an individual receives the **treatment** and 0 if **control**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the intercept and dummy variables\n",
    "\n",
    "ab_data['intercept'] = 1\n",
    "\n",
    "ab_data[['control','ab_page']] = pd.get_dummies(ab_data['group'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Use **statsmodels** to import your regression model.  Instantiate the model, and fit the model using the two columns you created in part **b.** to predict whether or not an individual converts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "# Fitting the log model to study the effect of landing page on conversion rate\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "log_model = sm.Logit(ab_data['converted'], ab_data[['ab_page','intercept']])\n",
    "\n",
    "results = log_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Provide the summary of your model below, and use it as necessary to answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of Results:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212780.3502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-06-16 15:44</td>       <td>BIC:</td>        <td>212801.5095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>1</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290582</td>        <td>LLR p-value:</td>      <td>0.18988</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>-0.0150</td>  <td>0.0114</td>   <td>-1.3109</td>  <td>0.1899</td> <td>-0.0374</td> <td>0.0074</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.9888</td>  <td>0.0081</td>  <td>-246.6690</td> <td>0.0000</td> <td>-2.0046</td> <td>-1.9730</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.000      \n",
       "Dependent Variable: converted        AIC:              212780.3502\n",
       "Date:               2021-06-16 15:44 BIC:              212801.5095\n",
       "No. Observations:   290584           Log-Likelihood:   -1.0639e+05\n",
       "Df Model:           1                LL-Null:          -1.0639e+05\n",
       "Df Residuals:       290582           LLR p-value:      0.18988    \n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "No. Iterations:     6.0000                                        \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "ab_page      -0.0150    0.0114    -1.3109  0.1899  -0.0374   0.0074\n",
       "intercept    -1.9888    0.0081  -246.6690  0.0000  -2.0046  -1.9730\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarizing results\n",
    "\n",
    "print('Summary of Results:\\n')\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse effect of ab_page: 1.015113064615719\n"
     ]
    }
   ],
   "source": [
    "# Calculating intepretation of variables\n",
    "\n",
    "inv_effect_of_ab_page  = 1/np.exp(-0.0150)\n",
    "print(f'Inverse effect of ab_page: {inv_effect_of_ab_page}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intepretation of result** \n",
    "\n",
    "Based on the coefficient of ab_page in the logistic regression the odds of conversion increases by a factor of 1.05 when a user is shown the old landing page as opposed to being shown the new landing page. \n",
    "\n",
    "The p-value associated with the result is greater than our type 1 error rate and therefore, we fail to reject the null hypothesis. This means we stick to the belief that the difference between the conversion rates of the two versions of the landing page is 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. What is the p-value associated with **ab_page**? Why does it differ from the value you found in **Part II**?<br><br>  **Hint**: What are the null and alternative hypotheses associated with your regression model, and how do they compare to the null and alternative hypotheses in the **Part II**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value associated with the ab_page is 0.1899. This is because the **null hypothesis** in **logistic regression** is \n",
    "that there is **no difference** in conversion rates for the two pages and the **alternative hypothesis** states that there **exists a difference** between the conversion rates of the two pages. \n",
    "\n",
    "This is different from the null and alternative hypothesis in part 2, where the null hypothesis is that the difference is less than zero and the alternative states that the difference is greater than zero. Since our observed value was less than zero, we got a p value of 90%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Now, you are considering other things that might influence whether or not an individual converts.  Discuss why it is a good idea to consider other factors to add into your regression model.  Are there any disadvantages to adding additional terms into your regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of more independant variables would result in a better fit, giving us more accurate predictions. \n",
    "The addition of these variables come at the risk of overfitting wherein we'll get a really high value of R² but the model \n",
    "would fail to capture the true relationship between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Now along with testing if the conversion rate changes for different pages, also add an effect based on which country a user lives. You will need to read in the **countries.csv** dataset and merge together your datasets on the approporiate rows.  [Here](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html) are the docs for joining tables. \n",
    "\n",
    "Does it appear that country had an impact on conversion?  Don't forget to create dummy variables for these country columns - **Hint: You will need two columns for the three dummy variables.** Provide the statistical output as well as a written response to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding country information to ab_data\n",
    "\n",
    "country_data.set_index(country_data['user_id'], inplace = True)\n",
    "ab_data = ab_data.join(country_data.country, on = ab_data['user_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the necessary dummy variables\n",
    "\n",
    "ab_data[['CA','UK','US']] = pd.get_dummies(ab_data['country'])\n",
    "ab_data.drop('CA', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366116\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212780.8333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-06-16 15:44</td>       <td>BIC:</td>        <td>212812.5723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>2</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290581</td>        <td>LLR p-value:</td>      <td>0.19835</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-2.0375</td>  <td>0.0260</td>  <td>-78.3639</td> <td>0.0000</td> <td>-2.0885</td> <td>-1.9866</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>        <td>0.0507</td>   <td>0.0284</td>   <td>1.7863</td>  <td>0.0740</td> <td>-0.0049</td> <td>0.1064</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>        <td>0.0408</td>   <td>0.0269</td>   <td>1.5178</td>  <td>0.1291</td> <td>-0.0119</td> <td>0.0935</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.000      \n",
       "Dependent Variable: converted        AIC:              212780.8333\n",
       "Date:               2021-06-16 15:44 BIC:              212812.5723\n",
       "No. Observations:   290584           Log-Likelihood:   -1.0639e+05\n",
       "Df Model:           2                LL-Null:          -1.0639e+05\n",
       "Df Residuals:       290581           LLR p-value:      0.19835    \n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "No. Iterations:     6.0000                                        \n",
       "-------------------------------------------------------------------\n",
       "               Coef.   Std.Err.     z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept     -2.0375    0.0260  -78.3639  0.0000  -2.0885  -1.9866\n",
       "UK             0.0507    0.0284    1.7863  0.0740  -0.0049   0.1064\n",
       "US             0.0408    0.0269    1.5178  0.1291  -0.0119   0.0935\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting a logistic regression model to study the effect of country\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "log_model_country = sm.Logit(ab_data['converted'], ab_data[['intercept','UK','US']] )\n",
    "result = log_model_country.fit()\n",
    "\n",
    "result.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in odds due to country being UK: 1.052007243765014\n",
      "Change in odds due to country being US: 1.0416437559600236\n"
     ]
    }
   ],
   "source": [
    "# Calulating intepretation of coefficients\n",
    "\n",
    "change_in_odds_UK = np.exp(0.0507)\n",
    "change_in_odds_US = np.exp(0.0408)\n",
    "\n",
    "print(f'Change in odds due to country being UK: {change_in_odds_UK}')\n",
    "print(f'Change in odds due to country being US: {change_in_odds_US}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intepretation of results\n",
    "\n",
    "Based on the model, the odds of a user converting is 1.05 time more in UK and 1.04 times more in US as compared to Canada. \n",
    "Suggesting that UK users are more likely to convert followed by US and then by Canada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. Though you have now looked at the individual factors of country and page on conversion, we would now like to look at an interaction between page and country to see if there significant effects on conversion.  Create the necessary additional columns, and fit the new model.  \n",
    "\n",
    "Provide the summary results, and your conclusions based on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining interaction variables\n",
    "\n",
    "ab_data['ab_page_with_UK'] = ab_data['UK']*ab_data['ab_page']\n",
    "ab_data['ab_page_with_US'] = ab_data['US']*ab_data['ab_page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n",
      "Summary of Results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212782.6602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2021-06-16 15:44</td>       <td>BIC:</td>        <td>212846.1381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>5</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290578</td>        <td>LLR p-value:</td>      <td>0.19199</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>          <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>     <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>       <td>-2.0040</td>  <td>0.0364</td>  <td>-55.0077</td> <td>0.0000</td> <td>-2.0754</td> <td>-1.9326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>         <td>-0.0674</td>  <td>0.0520</td>   <td>-1.2967</td> <td>0.1947</td> <td>-0.1694</td> <td>0.0345</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>UK</th>              <td>0.0118</td>   <td>0.0398</td>   <td>0.2957</td>  <td>0.7674</td> <td>-0.0663</td> <td>0.0899</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>              <td>0.0175</td>   <td>0.0377</td>   <td>0.4652</td>  <td>0.6418</td> <td>-0.0563</td> <td>0.0914</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page_with_UK</th> <td>0.0783</td>   <td>0.0568</td>   <td>1.3783</td>  <td>0.1681</td> <td>-0.0330</td> <td>0.1896</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page_with_US</th> <td>0.0469</td>   <td>0.0538</td>   <td>0.8718</td>  <td>0.3833</td> <td>-0.0585</td> <td>0.1523</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.000      \n",
       "Dependent Variable: converted        AIC:              212782.6602\n",
       "Date:               2021-06-16 15:44 BIC:              212846.1381\n",
       "No. Observations:   290584           Log-Likelihood:   -1.0639e+05\n",
       "Df Model:           5                LL-Null:          -1.0639e+05\n",
       "Df Residuals:       290578           LLR p-value:      0.19199    \n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "No. Iterations:     6.0000                                        \n",
       "------------------------------------------------------------------\n",
       "                   Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
       "------------------------------------------------------------------\n",
       "intercept         -2.0040   0.0364 -55.0077 0.0000 -2.0754 -1.9326\n",
       "ab_page           -0.0674   0.0520  -1.2967 0.1947 -0.1694  0.0345\n",
       "UK                 0.0118   0.0398   0.2957 0.7674 -0.0663  0.0899\n",
       "US                 0.0175   0.0377   0.4652 0.6418 -0.0563  0.0914\n",
       "ab_page_with_UK    0.0783   0.0568   1.3783 0.1681 -0.0330  0.1896\n",
       "ab_page_with_US    0.0469   0.0538   0.8718 0.3833 -0.0585  0.1523\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the logistic regression model to study the effect of interaction variables.\n",
    "\n",
    "log_model = sm.Logit(ab_data['converted'], ab_data[['intercept','ab_page','UK','US','ab_page_with_UK','ab_page_with_US']])\n",
    "results = log_model.fit()  \n",
    "\n",
    "print(\"Summary of Results\")\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse Effect of new page: 1.0697232819209153\n",
      "Effect of UK: 1.011869894648401\n",
      "Effect of US: 1.0176540221507617\n",
      "Effect of page and UK interaction: 1.0814470441230692\n",
      "Effect of page and US interaction: 1.048017202119183\n"
     ]
    }
   ],
   "source": [
    " # Calculating the intepretation of coefficients\n",
    "    \n",
    "effect_of_ab_page = 1/np.exp(-0.0674)\n",
    "effect_of_UK = np.exp(0.0118)\n",
    "effect_of_US = np.exp(0.0175)\n",
    "effect_of_ab_page_UK_interaction = np.exp(0.0783)\n",
    "effect_of_ab_page_US_interaction  = np.exp(0.0469)\n",
    "\n",
    "\n",
    "print(f'Inverse Effect of new page: {effect_of_ab_page}')\n",
    "print(f'Effect of UK: {effect_of_UK}')\n",
    "print(f'Effect of US: {effect_of_US}')\n",
    "print(f'Effect of page and UK interaction: {effect_of_ab_page_UK_interaction}')\n",
    "print(f'Effect of page and US interaction: {effect_of_ab_page_US_interaction}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intepretation of Results\n",
    "\n",
    "\n",
    "*Efect of page*  \n",
    "The odds of conversion decreases by a factor 1.07 when a user is shown the new page as opposed to being shown the old page. \n",
    "\n",
    "*Effect of country being UK*  \n",
    "The odds of conversion increases by a factor of 1.01 when a user is based in UK or US than when a user is based in Canada.\n",
    "\n",
    "*Effect of country being US*  \n",
    "The odds of conversion increases by a factor of 1.02 due to the interaction effects between the type of page and the location of UK.\n",
    "\n",
    "*Effect due to interaction effects between variables*  \n",
    "The odds of conversion increases by a factor of 1.05 due to the interaction effects between the type of page and the location of US. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Conclusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a two fold approach in deciding if the landing page should be upgraded. Based on the results from both of these approaches, I recommend that the company should **continue using the current landing page**. \n",
    "\n",
    "In the A/B Test approach, we stuck to our null hypothesis of the difference in conversion rate being less than or equal to zero. And in the Logistic Regression approach, we stuck to our null hypothesis that the difference is zero. \n",
    "\n",
    "In both cases, we can definitely conclude that the new landing page does not improve the conversion rate. Therefore it would be reasonable to avoid spending more money into developing & deploying the new landing page.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "\n",
    "\n",
    "Web Resources:\n",
    "\n",
    "Logistic Reression: https://stats.idre.ucla.edu/stata/output/logistic-regression-analysis/  \n",
    "Logistic Regression: Chomba Bupe's answer in - https://www.quora.com/What-is-the-mathematical-derivation-of-logistic-regression  \n",
    "Using the Built-in : https://stackoverflow.com/questions/52670195/using-statsmodels-ztest  \n",
    "For calcualting p-value: https://stackoverflow.com/questions/4406389/if-else-in-a-list-comprehension \n",
    "\n",
    "Books:\n",
    "\n",
    "Logistic Regression: Chapter 4 in Introduction to Statistical Learning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
